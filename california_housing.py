# -*- coding: utf-8 -*-
"""California Housing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iVLWMbg0jYxGqUF-XCWxsyohYmVZhwBp
"""

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score

# Load the datasets
train_data = pd.read_csv('sample_data/california_housing_train.csv')
test_data = pd.read_csv('sample_data/california_housing_test.csv')

# Prepare the features (X) and target (y)
X_train = train_data.drop('median_house_value', axis=1)
y_train = train_data['median_house_value']
X_test = test_data.drop('median_house_value', axis=1)
y_test = test_data['median_house_value']

# Create and train the Random Forest model
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)

# Create and train the Decision Tree model
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions on the test set
rf_predictions = rf_model.predict(X_test)
dt_predictions = dt_model.predict(X_test)

# Calculate the mean squared error for both models
rf_mse = mean_squared_error(y_test, rf_predictions)
dt_mse = mean_squared_error(y_test, dt_predictions)

# Calculate RMSE
rf_rmse = rf_mse ** 0.5
dt_rmse = dt_mse ** 0.5

# Displaying the RMSE results in a DataFrame
results_df = pd.DataFrame({
    'Model': ['Random Forest', 'Decision Tree'],
    'RMSE': [rf_rmse, dt_rmse]
})

print(results_df)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create and train the SVR model
svr_model = SVR(kernel='linear')
svr_model.fit(X_train_scaled, y_train)

# Predict on the scaled testing data
svr_predictions = svr_model.predict(X_test_scaled)

# Calculate the mean squared error and RMSE
svr_mse = mean_squared_error(y_test, svr_predictions)
svr_rmse = svr_mse ** 0.5

# Print RMSE
print("RMSE for SVR Model:", svr_rmse)

from sklearn.model_selection import GridSearchCV

# Set up the SVR with GridSearch to tune parameters
parameters = {
    'C': [1, 10, 100],  # Regularization parameter
    'gamma': ['scale', 'auto'],  # Kernel coefficient
    'kernel': ['rbf']  # Using the radial basis function kernel
}
svr = SVR()
grid_search = GridSearchCV(svr, parameters, cv=3, scoring='neg_mean_squared_error', verbose=2)
grid_search.fit(X_train_scaled, y_train)

# Best estimator found by GridSearch
best_svr = grid_search.best_estimator_

# Predict on the scaled testing data
svr_predictions = best_svr.predict(X_test_scaled)

# Calculate the mean squared error and RMSE
svr_mse = mean_squared_error(y_test, svr_predictions)
svr_rmse = svr_mse ** 0.5

# Print RMSE and best parameters
print("RMSE for Optimized SVR Model:", svr_rmse)
print("Best Parameters:", grid_search.best_params_)

nn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1)
])

# Compile the Neural Network model
nn_model.compile(optimizer='adam', loss='mean_squared_error')

# Train the Neural Network model
nn_model.fit(X_train_scaled, y_train, epochs=10, validation_split=0.1)

# Evaluate the Neural Network model on the test set
nn_mse = nn_model.evaluate(X_test_scaled, y_test)
nn_rmse = nn_mse ** 0.5
print("RMSE for Neural Network Model:", nn_rmse)

# Random Forest
rf_model = RandomForestRegressor(random_state=42)
rf_scores = cross_val_score(rf_model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')
rf_rmse_scores = np.sqrt(-rf_scores)
print("Random Forest RMSE for each fold:", rf_rmse_scores)
print("Average RMSE over all folds:", rf_rmse_scores.mean())

# Decision Tree
dt_model = DecisionTreeRegressor(random_state=42)
dt_scores = cross_val_score(dt_model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')
dt_rmse_scores = np.sqrt(-dt_scores)
print("Decision Tree RMSE for each fold:", dt_rmse_scores)
print("Average RMSE over all folds:", dt_rmse_scores.mean())

# SVR - Use scaled features
svr_model = SVR(kernel='linear')
svr_scores = cross_val_score(svr_model, X_train_scaled, y_train, cv=10, scoring='neg_mean_squared_error')
svr_rmse_scores = np.sqrt(-svr_scores)
print("SVR RMSE for each fold:", svr_rmse_scores)
print("Average RMSE over all folds:", svr_rmse_scores.mean())



correlation_matrix = train_data.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})
plt.title('Correlation Matrix of Housing Features')
plt.show()

# Binning the output
bins = [0, 100000, 300000, np.inf]
labels = ['Low', 'Medium', 'High']
y_train_binned = pd.cut(train_data['median_house_value'], bins=bins, labels=labels, right=False)
y_test_binned = pd.cut(test_data['median_house_value'], bins=bins, labels=labels, right=False)

# Convert predictions to categories (using RF model predictions as example)
predicted_categories = pd.cut(rf_predictions, bins=bins, labels=labels, right=False)

# Generate the confusion matrix
cm = confusion_matrix(y_test_binned, predicted_categories, labels=labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix for Housing Price Categories')
plt.show()

# Plotting Predicted vs. Actual values for the Neural Network
predictions = nn_model.predict(X_test_scaled).flatten()
plt.figure(figsize=(10, 6))
plt.scatter(y_test, predictions, alpha=0.3)
plt.title('Predicted vs. Actual Values')
plt.xlabel('Actual Median House Values')
plt.ylabel('Predicted Median House Values')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')  # Line showing perfect predictions
plt.grid(True)
plt.show()

# Plotting Residuals for the Neural Network
residuals = y_test - predictions
plt.figure(figsize=(10, 6))
plt.scatter(predictions, residuals, alpha=0.3)
plt.title('Residuals Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.hlines(y=0, xmin=predictions.min(), xmax=predictions.max(), colors='red')
plt.grid(True)
plt.show()

